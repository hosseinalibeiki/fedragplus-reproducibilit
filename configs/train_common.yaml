model:
  generator_name: "meta-llama/Llama-3.2-3B-Instruct"   
  finetune:
    method: "lora"        
    lora_r: 8
    lora_alpha: 16
    lora_dropout: 0.05
  generation:
    max_new_tokens: 128
    temperature: 0.7
    top_p: 0.9

optim:
  optimizer: "adamw"
  lr: 3.0e-5
  weight_decay: 0.01
  grad_clip_norm: 1.0
  warmup_ratio: 0.06
  lr_schedule: "linear"

compute:
  device: "cuda"
  fp16: true
  gradient_accumulation_steps: 1
